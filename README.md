# Awesome Hallucination Papers in MLLMs
A curated list of papers about hallucination in multi-modal large language models (MLLMs)

## Survey Papers
This section collects the survey papers about MLLM's hallucination.

- **A Survey on Hallucination in Large Vision-Language Models** [[paper]](https://arxiv.org/pdf/2402.00253v1.pdf)

  `Arxiv 2024/02`

## Benchmark Papers
This section collects the benchmark papers on evaluating MLLM's hallucination.

- **Evaluating Object Hallucination in Large Vision-Language Models** [[paper]](https://arxiv.org/pdf/2305.10355.pdf) [[code]](https://github.com/RUCAIBox/POPE)

  `EMNLP 2023`

- **HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models** [[paper]](https://arxiv.org/pdf/2310.14566.pdf) [[code]](https://github.com/tianyi-lab/HallusionBench)

  `CVPR 2024`

- *Aligning Large Multimodal Models with Factually Augmented RLHF* [[paper]](https://arxiv.org/pdf/2309.14525.pdf) [[code]](https://github.com/llava-rlhf/LLaVA-RLHF)

  `Arxiv 2023/09`

- *An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation* [[paper]](https://arxiv.org/pdf/2311.07397.pdf) [[code]](https://github.com/junyangwang0410/AMBER)

  `Arxiv 2023/11`

- *Holistic Analysis of Hallucination in GPT-4V(ision): Bias and Interference Challenges* [[paper]](https://arxiv.org/pdf/2311.03287.pdf) [[code]](https://github.com/gzcch/Bingo)

  `Arxiv 2023/11`

- *Hallucination Benchmark in Medical Visual Question Answering* [[paper]](https://arxiv.org/pdf/2401.05827v1.pdf)

  `Arxiv 2024/01`

- *The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs* [[paper]](https://arxiv.org/pdf/2402.03757v1.pdf) [[code]](https://github.com/MasaiahHan/CorrelationQA)

  `Arxiv 2024/02`

- *Unified Hallucination Detection for Multimodal Large Language Models* [[paper]](https://arxiv.org/pdf/2402.03190v1.pdf) [[code]](https://github.com/OpenKG-ORG/EasyDetect)

  `Arxiv 2024/02`

- *Visual Hallucinations of Multi-modal Large Language Models* [[paper]](https://arxiv.org/pdf/2402.14683v1.pdf) [[code]](https://github.com/wenhuang2000/VHTest)

  `Arxiv 2024/02`

- *Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models* [[paper]](https://arxiv.org/pdf/2402.15721v1.pdf)

  `Arxiv 2024/02`

- *PhD: A Prompted Visual Hallucination Evaluation Dataset* [[paper]](https://arxiv.org/pdf/2403.11116v1.pdf) [[code]](https://github.com/jiazhen-code/IntrinsicHallu)

  `Arxiv 2024/03`

- *Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models* [[paper]](https://arxiv.org/pdf/2403.20331v1.pdf) [[code]](https://github.com/AtsuMiyai/UPD/)

  `Arxiv 2024/04`

- *THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models* [[paper]](https://arxiv.org/pdf/2405.05256v1)

  `Arxiv 2024/05`

## Hallucination Mitigation
This section collects the papers on mitigating the MLLM's hallucination.

- **Mitigating Object Hallucination via Data Augmented Contrastive Tuning** [[paper]](https://arxiv.org/abs/2405.18654)
  
  `Arxiv 2024/05`

- **Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning** [[paper]](https://arxiv.org/pdf/2306.14565.pdf) [[code]](https://github.com/FuxiaoLiu/LRV-Instruction)

  `ICLR 2024`

- **Analyzing and Mitigating Object Hallucination in Large Vision-Language Models** [[paper]](https://arxiv.org/pdf/2310.00754.pdf) [[code]](https://github.com/YiyangZhou/LURE)

  `ICLR 2024`

- **VIGC: Visual Instruction Generation and Correction** [[paper]](https://arxiv.org/pdf/2308.12714.pdf)[[code]](https://github.com/opendatalab/VIGC)
  
  `AAAI 2024`

- **OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation** [[paper]](https://arxiv.org/pdf/2311.17911.pdf) [[code]](https://github.com/shikiw/OPERA)

  `CVPR 2024`

- **Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding** [[paper]](https://arxiv.org/pdf/2311.16922.pdf) [[code]](https://github.com/DAMO-NLP-SG/VCD)

  `CVPR 2024`

- **Hallucination Augmented Contrastive Learning for Multimodal Large Language Model** [[paper]](https://arxiv.org/pdf/2312.06968.pdf) 

  `CVPR 2024`

- **RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback** [[paper]](https://arxiv.org/pdf/2312.00849.pdf) [[code]](https://github.com/RLHF-V/RLHF-V)

  `CVPR 2024`

- *Detecting and Preventing Hallucinations in Large Vision Language Models* [[paper]](https://arxiv.org/pdf/2308.06394.pdf) 

  `Arxiv 2023/08`

- *Evaluation and Analysis of Hallucination in Large Vision-Language Models* [[paper]](https://arxiv.org/pdf/2308.15126.pdf)[[code]](https://github.com/junyangwang0410/HaELM)
  
  `Arxiv 2023/08`

- *CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning* [[paper]](https://arxiv.org/pdf/2308.15126.pdf)
  
  `Arxiv 2023/09`

- *Evaluation and Mitigation of Agnosia in Multimodal Large Language Models* [[paper]](https://arxiv.org/pdf/2309.04041.pdf)
  
  `Arxiv 2023/09`

- *Aligning Large Multimodal Models with Factually Augmented RLHF* [[paper]](https://arxiv.org/pdf/2309.14525.pdf) [[code]](https://github.com/llava-rlhf/LLaVA-RLHF)

  `Arxiv 2023/09`

- *HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption* [[paper]](https://arxiv.org/pdf/2310.01779.pdf)
  
  `Arxiv 2023/10`

- *Woodpecker: Hallucination Correction for Multimodal Large Language Models* [[paper]](https://arxiv.org/pdf/2310.16045.pdf) [[code]](https://github.com/BradyFU/Woodpecker)

  `Arxiv 2023/10`

- *HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data* [[paper]](https://arxiv.org/pdf/2311.13614.pdf) [[code]](https://github.com/Yuqifan1117/HalluciDoctor)

  `Arxiv 2023/11`

- *VOLCANO: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision* [[paper]](https://arxiv.org/pdf/2311.07362.pdf) [[code]](https://github.com/kaistAI/Volcano)

  `Arxiv 2023/11`

- *Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization* [[paper]](https://arxiv.org/pdf/2311.16839.pdf) 

  `Arxiv 2023/11`

- *Mitigating Hallucination in Visual Language Models with Visual Supervision* [[paper]](https://arxiv.org/pdf/2311.16479.pdf) 

  `Arxiv 2023/11`

- *Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites* [[paper]](https://arxiv.org/pdf/2312.01701.pdf) [[code]](https://github.com/Anonymousanoy/FOHE)

  `Arxiv 2023/12`

- *MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations* [[paper]](https://arxiv.org/pdf/2312.03631.pdf) [[code]](https://github.com/assafbk/mocha_code)

  `Arxiv 2023/12`

- *Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models* [[paper]](https://arxiv.org/pdf/2401.09861v1.pdf) 

  `Arxiv 2024/01`

- *On the Audio Hallucinations in Large Audio-Video Language Models* [[paper]](https://arxiv.org/pdf/2401.09774v1.pdf) 

  `Arxiv 2024/01`

- *Skip \n: A simple method to reduce hallucination in Large Vision-Language Models* [[paper]](https://arxiv.org/pdf/2402.01345v1.pdf) 

  `Arxiv 2024/02`

- *Unified Hallucination Detection for Multimodal Large Language Models* [[paper]](https://arxiv.org/pdf/2402.03190v1.pdf) [[code]](https://github.com/OpenKG-ORG/EasyDetect)

  `Arxiv 2024/02`

- *Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance* [[paper]](https://arxiv.org/pdf/2402.08680v1.pdf) 

  `Arxiv 2024/02`

- *EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models* [[paper]](https://arxiv.org/pdf/2402.09801v1.pdf) 

  `Arxiv 2024/02`

- *Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models* [[paper]](https://arxiv.org/pdf/2402.11622v1.pdf) [[code]](https://github.com/Hyperwjf/LogicCheckGPT)

  `Arxiv 2024/02`

- *Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective* [[paper]](https://arxiv.org/pdf/2402.14545v1.pdf) [[code]](https://github.com/yuezih/less-is-more)

  `Arxiv 2024/02`

- *Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding* [[paper]](https://arxiv.org/pdf/2402.15300v1.pdf)

  `Arxiv 2024/02`

- *IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding* [[paper]](https://arxiv.org/pdf/2402.18476v1.pdf)

  `Arxiv 2024/02`

- *HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding* [[paper]](https://arxiv.org/pdf/2403.00425v1.pdf) [[code]](https://github.com/BillChan226/HALC)

  `Arxiv 2024/03`

- *Evaluating and Mitigating Number Hallucinations in Large Vision-Language Models: A Consistency Perspective* [[paper]](https://arxiv.org/pdf/2403.01373v1.pdf)

  `Arxiv 2024/03`

- *Debiasing Large Visual Language Models* [[paper]](https://arxiv.org/pdf/2403.05262.pdf)

  `Arxiv 2024/03`

- *AIGCs Confuse AI Too: Investigating and Explaining Synthetic Image-induced Hallucinations in Large Vision-Language Models* [[paper]](https://arxiv.org/pdf/2403.08542v1.pdf)

  `Arxiv 2024/03`

- *What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models* [[paper]](https://arxiv.org/pdf/2403.13513v1.pdf)

  `Arxiv 2024/03`

- *Multi-Modal Hallucination Control by Visual Information Grounding* [[paper]](https://arxiv.org/pdf/2403.14003v1.pdf)

  `Arxiv 2024/03`

- *Pensieve: Retrospect-then-Compare Mitigates Visual Hallucination* [[paper]](https://arxiv.org/pdf/2403.14401v1.pdf) [[code]](https://github.com/DingchenYang99/Pensieve)

  `Arxiv 2024/03`

- *Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art* [[paper]](https://arxiv.org/pdf/2403.16527v1.pdf)

  `Arxiv 2024/03`

- *Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning* [[paper]](https://arxiv.org/pdf/2403.15048v2.pdf)

  `Arxiv 2024/03`

- *Visual Hallucination: Definition, Quantification, and Prescriptive Remediations* [[paper]](https://arxiv.org/pdf/2403.17306v1.pdf)

  `Arxiv 2024/03`

- *Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models* [[paper]](https://arxiv.org/pdf/2403.16167v2.pdf)

  `Arxiv 2024/03`

- *Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding* [[paper]](https://arxiv.org/pdf/2403.18715v1.pdf)

  `Arxiv 2024/03`

- *Automated Multi-level Preference for MLLMs* [[paper]](https://www.arxiv.org/pdf/2405.11165)

  `Arxiv 2024/05`

- *CrossCheckGPT: Universal Hallucination Ranking for Multimodal Foundation Models* [[paper]](https://arxiv.org/pdf/2405.13684v1)

  `Arxiv 2024/05`

- *VDGD: Mitigating LVLM Hallucinations in Cognitive Prompts by Bridging the Visual Perception Gap* [[paper]](http://arxiv.org/pdf/2405.15683v1)

  `Arxiv 2024/05`

- *Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization* [[paper]](https://arxiv.org/pdf/2405.15356v1)

  `Arxiv 2024/05`

- *Mitigating Dialogue Hallucination for Large Vision Language Models via Adversarial Instruction Tuning* [[paper]](https://arxiv.org/pdf/2403.10492v2)

  `Arxiv 2024/05`

- *RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs* [[paper]](https://arxiv.org/pdf/2405.17821v1)

  `Arxiv 2024/05`

- *MetaToken: Detecting Hallucination in Image Descriptions by Meta Classification* [[paper]](https://arxiv.org/pdf/2405.19186v1)

  `Arxiv 2024/05`

- *Mitigating Object Hallucination via Data Augmented Contrastive Tuning* [[paper]](https://arxiv.org/pdf/2405.18654v1)

  `Arxiv 2024/05`

- *NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models* [[paper]](https://arxiv.org/pdf/2405.20081v2) [[code]](https://kaiwu5.github.io/noiseboost)

  `Arxiv 2024/06`

- *CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models* [[paper]](https://arxiv.org/pdf/2406.01920v1) [[code]](https://ivy-lvlm.github.io/CODE/)

  `Arxiv 2024/06`

- *Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models* [[paper]](http://arxiv.org/pdf/2406.08402v1)

  `Arxiv 2024/06`
